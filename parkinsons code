# CELL 1 - Required imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import tensorflow as tf
import joblib

# CELL 2 - Create and save sklearn model
def train_sklearn_model():
    # Load data
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data"
    df = pd.read_csv(url)

    # Prepare data
    X = df.drop(['name', 'status'], axis=1)
    y = df['status']

    # Create and fit pipeline
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_scaled, y)

    # Save components
    joblib.dump(scaler, 'scaler.joblib')
    joblib.dump(model, 'rf_model.joblib')
    joblib.dump(list(X.columns), 'feature_names.joblib')

    return scaler, model, list(X.columns)

# CELL 3 - Create TensorFlow model
def create_tf_model(feature_count):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(feature_count,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# CELL 4 - Transfer sklearn predictions to tensorflow model
def transfer_to_tf_model(sklearn_model, scaler, feature_names):
    # Create sample data
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data"
    df = pd.read_csv(url)
    X = df.drop(['name', 'status'], axis=1)
    y = df['status']

    # Scale data
    X_scaled = scaler.transform(X)

    # Get predictions from sklearn model
    y_pred = sklearn_model.predict_proba(X_scaled)

    # Create and train TF model
    tf_model = create_tf_model(len(feature_names))
    tf_model.fit(X_scaled, y, epochs=10, verbose=0)  # Train briefly to initialize

    # Save the TF model
    tf_model.save('parkinsons_model.h5')

    # Convert to TFLite
    converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)
    tflite_model = converter.convert()

    # Save TFLite model
    with open('parkinsons_model.tflite', 'wb') as f:
        f.write(tflite_model)

    return tf_model

# CELL 5 - Main execution
print("Training sklearn model...")
scaler, sklearn_model, feature_names = train_sklearn_model()

print("Converting to TensorFlow Lite...")
tf_model = transfer_to_tf_model(sklearn_model, scaler, feature_names)

print("Model conversion completed successfully!")

# CELL 6 - Test the TFLite model
def test_tflite_model():
    # Load and prepare test data
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data"
    df = pd.read_csv(url)
    X = df.drop(['name', 'status'], axis=1)[:1]  # Take first sample
    X_scaled = scaler.transform(X)

    # Load TFLite model
    interpreter = tf.lite.Interpreter(model_path="parkinsons_model.tflite")
    interpreter.allocate_tensors()

    # Get input and output tensors
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Test prediction
    input_data = X_scaled.astype(np.float32)
    interpreter.set_tensor(input_details[0]['index'], [input_data[0]])
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])

    print("\nTest prediction successful!")
    print("Output shape:", output_data.shape)
    print("Prediction probabilities:", output_data[0])

# Test the converted model
test_tflite_model()
